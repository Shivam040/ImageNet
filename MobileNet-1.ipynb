{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input, DepthwiseConv2D\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization\n",
    "from tensorflow.keras.layers import ReLU, Flatten, Dense, AvgPool2D\n",
    "from tensorflow.keras import Model\n",
    "import os\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the ImageNet 32x32 dataset\n",
    "dataset_name = \"imagenet_resized/32x32\"\n",
    "dataset, info = tfds.load(dataset_name, split=['train', 'validation'], as_supervised=True, with_info=True)\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_dataset = dataset[0]\n",
    "test_dataset = dataset[1]\n",
    "\n",
    "# Display dataset info\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list, label_list = [], []\n",
    "\n",
    "for image, label in tfds.as_numpy(dataset[0]):\n",
    "    image_list.append(image)\n",
    "    label_list.append(label)\n",
    "\n",
    "image_array = np.array(image_list)\n",
    "label_array = np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stratified sampling (80% train, 20% validation)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    image_array, label_array, test_size=0.2, stratify=label_array, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to TensorFlow dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and batch\n",
    "batch_size = 32\n",
    "train_dataset = train_dataset.shuffle(len(train_images)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Check dataset sizes\n",
    "print(f\"Train Size: {len(train_images)}, Validation Size: {len(val_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobilnet_block (x, filters, strides):\n",
    "    \n",
    "    x = DepthwiseConv2D(kernel_size = 3, strides = strides, padding = 'same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Conv2D(filters = filters, kernel_size = 1, strides = 1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "input = Input(shape = (32,32,3))\n",
    "x = Conv2D(filters = 32, kernel_size = 3, strides = 1, padding = 'same')(input)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "\n",
    "x = mobilnet_block(x, filters = 64, strides = 1)\n",
    "x = mobilnet_block(x, filters = 128, strides = 1)\n",
    "x = mobilnet_block(x, filters = 128, strides = 2)\n",
    "x = mobilnet_block(x, filters = 256, strides = 2)\n",
    "x = mobilnet_block(x, filters = 512, strides = 1)\n",
    "x = mobilnet_block(x, filters = 512, strides = 2)\n",
    "for _ in range (5):\n",
    "    x = mobilnet_block(x, filters = 512, strides = 1)\n",
    "x = mobilnet_block(x, filters = 1024, strides = 2)\n",
    "x = mobilnet_block(x, filters = 1024, strides = 1)\n",
    "# x = AvgPool2D ()(x)\n",
    "x = Flatten()(x)\n",
    "output = Dense (units = 1000, activation = 'softmax')(x)\n",
    "model = Model(inputs=input, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.00003),\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint('MobileNet_2.h5', save_best_only=True, monitor=\"val_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using PrefetchDataset\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=10, callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.00002),\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=20, callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_contrast(image, 0.8, 1.2)\n",
    "    image = tf.image.random_brightness(image, 0.2)\n",
    "    image = tf.image.resize_with_crop_or_pad(image, 36, 36)  # Add padding\n",
    "    image = tf.image.random_crop(image, [32, 32, 3])  # Crop back to 32x32\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply augmentation to the dataset\n",
    "augment_train_dataset = train_dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "augment_train_dataset = augment_train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005),\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "# Train the model\n",
    "training = model.fit(\n",
    "    augment_train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=20,\n",
    "    callbacks=[model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=5,\n",
    "    callbacks=[model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # Adjust based on available memory\n",
    "test_dataset = test_dataset.batch(batch_size)  # Add batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join('models','MobileNet.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cifer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
